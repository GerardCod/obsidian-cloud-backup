#GCP #Containers 

Containers are an approach to virtualization where you consolidate everything you need to run an application into an executable software package. This includes the code, runtime, system tools, system libraries, and configurations. An important thing to note is because containers share the operating system with their host machine, they are very lightweight, resource-efficient, and portable. This stands in contrast to VMs since they emulate physical machines and therefore each VM requires an operating system.

Containers also pair well with service or microservice-based applications. In the world of on-premises environments and physical machines, an application was built with what was called **monolithic architecture**. This meant that application code was written like a novel, with all of the different components woven together to deliver a set of features and capabilities. This approach to building systems and applications can create issues around scalability, availability, and cost. Because monolithic applications are one large code base, they would also require a significant amount of hardware to be provisioned to a server for it to run and perform.

This approach can inhibit innovation by reducing the release velocity of development teams, given that making changes to a large code base can have unintended consequences and downstream impacts on the code. Changes are very scary to make and can have significant implications on revenue.

By contrast, when you take a service- or microservice-based approach to application development, you break an application down into smaller, more manageable, and independent components. This makes it significantly less risky to launch new features and capabilities given that the different components aren’t woven together.

These smaller bits of code that are independent of each other still need to work together to provide a holistic experience to clients. These bits of code are packaged in containers, with all of the libraries and dependencies it needs to run, and then deployed in groups called Pods. A Pod is composed of all of the components required to run an application for an end user. Pods can run single or multiple containers that are needed to deliver a specific set of services through an application. The “one-container-per-Pod” model is very common for Kubernetes use cases.

When pods run multiple containers, they are typically separate services that need to work together in order to deliver a specific experience within an application. This a more advanced use case for container orchestration but becomes more common as applications are broken down into services and micro services.

Google Cloud provides CaaS to clients through its console and abstracts away the complexity of managing operating systems. In the world of CaaS, clients may still want to manage the runtime environment itself but don’t need the burden of provisioning specific VMs and their operating systems. This enables additional value and functionality for them, such as autoscaling infrastructure, which can optimize cost relative to traffic while ensuring applications aren’t overloaded with traffic relative to their current hardware capacity.

The orchestration of containers, their Pods, and the underlying infrastructure is done by an open source technology called Kubernetes. **Kubernetes** is a service that makes it easy for the technology team to provision and manage VMs at scale with automation. Kubernetes can kill VMs that are dysfunctional while ensuring that duplicate nodes and pods can handle the traffic without impacting the customer experience. It can also scale the clusters based on alerts and traffic signals.