#GCP #Data 

The first step of manipulating data in the cloud is to upload the data to a cloud environment. Depending on the source system and the volume of data, different approaches can be taken. For small file uploads, for example, it may be as simple as using the user interface of Google Cloud to upload a file to a storage bucket. However, as data volumes grow, this simple GUI-based upload may not make sense. Google Cloud customers can also upload data to the cloud via a couple of methods: a transfer over the internet or via a transfer appliance.

The most convenient way of transferring data is generally doing so over the internet, and typically, this is the preferred method. Assuming there are reasons why you can’t do the transfer online, either because of bandwidth limitations or cost, customers can opt to do a transfer via hardware. For the transfer appliance, customers receive a physical storage server that is meant to be hydrated with corporate data that will be uploaded to the cloud.

These methods sound great if you’re working with static data, but what if you need to update it relatively frequently, such as daily or monthly, or even need real-time data? That’s where data pipelines come into play. An **ingestion pipeline** is a process where a system calls data from a source system and lands it in a destination system. This can be done autonomously and at scale via APIs as part of an automated ingestion pipeline.

The primary way to ingest data into Google Cloud when building automated data pipelines is with Pub/Sub. **Pub/Sub** is a serverless autoscaling data ingestion service that is great for ingesting data from multiple sources and is able to land the data across multiple services. For example, Pub/Sub may call APIs from a **customer relationship management system** (**CRM**) and a marketing system and land that data in a Cloud Storage bucket for archival or retention purposes. It may also push the data to a transformation pipeline where it will be sanitized before arriving at its final destination: a data warehouse where it can be consumed by other systems or employees for a specific business purpose. Some advantages of Pub/Sub include its **high availability** (**HA**) across regions, at-least-once message delivery with support for both push and pull modes, and its ability to handle real-time data.

Sometimes, customers may depend on a third-party or open source version of the ingesting solution, and the most common one is **Apache Kafka**. Kafka is an open source distributed streaming platform that is used to ingest large volumes of data from multiple systems. It can also be used or procured through **independent software vendors** (**ISVs**) who offer managed versions of Kafka such as Confluent and Aiven, which offer a hardened version of the software with some level of infrastructure abstraction where clusters can scale based on need or load.

Once data is in the cloud, ingestion services will typically land the data in a transformation pipeline where the data is cleaned and structured in a way that will be in harmony with the rest of the data in a datastore. Alternatively or simultaneously, the ingested data may need to be stored in a data lake where it may be retained in its raw format for retention or even compliance-related purposes.

A **data lake** in the world of cloud computing is a central repository of data that is used to store large volumes of data in its native format. Data lakes are typically built on top of object storage services. **Google Cloud Storage** (**GCS**) is an example of an object storage service, and the benefit of this type of storage is that it can handle any file type, be it structured, semi-structured, or unstructured data. Beyond being flexible in that it can store all file types, cloud-based object storage buckets are also scalable and cost-effective when compared to other datastores. The size of storage buckets can be dynamic and grow or shrink relative to how much data is in the bucket. Companies don’t have to provision a certain number of servers or compute capacity to address the growth of the data; it’s purely a storage service with no compute component, allowing for very cost-effective data storage.

Let’s assume that Pub/Sub sent data to both a data lake for long-term retention and a data pipeline for processing. Once the data is forked and sent to a transformation pipeline, this will typically kick off an **extract, transform, and load** (**ETL**) or **extract, load, and transform** (**ELT**) process. These acronyms denote whether data is transformed before or after landing at its destination. Within Google Cloud, the most common services used for ETL jobs are **Dataflow** and **Dataproc**. We’ll go deeper into those two services later on in the chapter, but for now, we should think of Dataflow as a serverless Apache Beam service that handles both real-time and batch transformation jobs. Meanwhile, Dataproc can be thought of as serverless Apache Spark. Apache Beam and Spark are both open source projects for data processing.

Once data is cleaned through an automated data pipeline via Dataflow or Dataproc, for example, it is then landed in one or multiple systems for consumption. If this data is externally facing or is business-critical data for business operations, it may land in a database. If the data is intended to be used by data science, business analyst teams, or visualization, it may land in a data warehouse.

A database is typically defined as a collection of data within a system that is organized and structured in a way that makes it easily searchable and accessible to other systems. In the world of the cloud, the traditional relational database has grown into several different types of datastores based on the type of data and the needs of an application. **Relational databases** are what people typically think of when they think of databases. They store data in tables composed of columns and rows, where each row represents a single record and each column represents a piece of data about that record. A great example of relational data is data that is used as part of a personal banking application. Your account and balance information is highly sensitive and must always be accurate. Adding or removing a 0 from your account balance can have a drastic impact on your financial health. Therefore, this data must be treated with the utmost care to ensure it is consistently accurate and able to change, with an audit trail, based on charges you incur in real time.

Relational databases continue to be used in cloud computing where they make sense. One of the constraints on these kinds of databases is that they must maintain data integrity or be ACID compliant, which means that they can normally only scale vertically—by adding more compute and storage to an existing database. **ACID** is an acronym that stands for **atomicity, consistency, isolation, and durability**. It represents properties that ensure database transactions are processed reliably. When deploying a database that is ACID compliant, you can guarantee that the data in the database will be accurate and reliable, particularly when best practices are adopted, such as backups in order to minimize the impact of unexpected issues.

Maintaining large relational databases can be very expensive given the continuous need to scale the hardware that it’s running on, and they struggle to handle global use cases. An example of using a relational database that makes sense is as the backend for a banking application for a small, regional bank. Bank account information must be consistent, durable, and isolated. If, for example, you wake up one day and it turns out the bank accidentally withdrew $10,000 instead of $1,000, it may cause you to miss payments on a mortgage or a car loan, which is unacceptable. By keeping banking data in a relational database, the regional bank can ensure data integrity despite potential errors or failures such as system outages.

Google Cloud has developed a relational database that can overcome some of the challenges associated with relational databases. **Spanner** is able to scale horizontally or across multiple nodes, while still retaining its ACID compliance. We’ll go deeper into what this means later in the chapter, but for now, it’s important to be aware that not all relational databases are created equal. Customers who opt for a traditional relational database on Google Cloud will typically use the Cloud SQL service, which supports MySQL and PostgreSQL, both open source projects, or Microsoft SQL Server.

When use cases arise where a relational database won’t be appropriate for the application, either because the data is not structured properly or the cost-to-performance ratios don’t make sense, for example, customers can use non-relational databases.

**Non-relational databases** or **NoSQL databases** have a different way of storing and organizing data for retrieval. Rather than storing data in tables with columns and rows, data may be stored as key-value pairs, documents, or graphs. A **key-value datastore** is used to describe a system that uses a key as a unique identifier with which a plethora of other data can be stored across a variety of formats. An example of when this makes sense is gaming session data. The username becomes the key, and values that are paired with the key may be current health, world map location, item skins, and statistics associated with the current game state. They are generally great for caching, session management, and storing simple data. A great example of a key-value pair datastore is Memorystore, which is Google Cloud’s managed offering of Redis. Redis is often used to store game and session data in gaming applications. The key would be the user ID, and the values associated with the key could be location on a map, what items are in the inventory, and how much health a character has at a specific point in time in the game.

Document databases are great for storing semi-structured data such as documents. They tend to be more flexible than key-value datastores, but they’re also not as fast at retrieving and storing data. They can also be searched based on the content of the documents rather than by just the key. They make great backends for document repositories or storing semi-structured data such as JSON documents. **Firestore** is a great example of a document-oriented database that is able to handle real-time data and is easy to get up and running. It’s fully serverless, allowing it to scale up and down to meet demand, which makes it a great backend for semi-structured search use cases such as searching inside documents.

**Graph databases** store data in a network of nodes and edges known as a graph. Each node represents an entity, while each edge represents a relation between two entities. When there are multiple entities with complex relationships to each other, graph databases are optimal for addressing the use case. They are very common when addressing the needs of social networking applications, fraud detection systems, and recommendation engines. This is powerful when recommending content to users as graph databases are able to inform the engine on the nuances of a person’s interests. Knowing that a user was born in Boston and has friends who like football, it may recommend content on the _New England Patriots_ for that user.

Both relational and non-relational databases are great ways to store data and make sense to implement based on the specific needs of the business, application, or customer. We typically think of data in a database as _hot_ data or data that needs to be readily accessible at any moment. That data may need to be accessible by a customer, such as checking bank account information, or an employee who is searching for a legal document that outlines a specific policy.

Assuming data needs to be made available to a data science team, however, databases may not be the most efficient or cost-effective way to store data. When we get into the world of big data, where data scientists may need access to a petabyte of data if not more, databases may not be able to handle the scale. Between being able to serve vast amounts of data and the cost pressure from running traditional datastores, the concept of a data warehouse was born.

A **data warehouse** is a central repository of ideally clean data that can be used for analysis, reporting, and ML use cases. It is typically architected in a way that makes it highly scalable while still being able to quickly and efficiently perform queries. Google Cloud’s **BigQuery**, for example, has decoupled the compute and storage components of the technology, allowing for the storage to scale dynamically while only leveraging compute as needed by queries. BigQuery also allows data teams to partition the data, allowing for queries to run on only parts of the datasets in the system, which also significantly improves query speed while minimizing cost. Warehouses are also able to serve as a landing or staging zone for chaotic data. Customers can land data in the warehouse before transforming it, known as an ELT process. This flexibility makes them great at handling semi-structured data and as a staging step for production data.

Because storing data in data warehouses is very cost-efficient, customers can marry datasets that previously would’ve been isolated due to a lack of system integration or cross-team collaboration. If you are a software company, you can enrich historical sales data with marketing campaign information, real-time sales data, customer surveys, and third-party datasets. This big data approach to understanding a business can be really powerful as leadership can make decisions based on a deeper, richer, data-driven lens rather than one clouded by stale, disjointed, or erroneous data.

Data warehouses also make great backends for visualization tools such as Google Cloud’s **Looker**. Given the variety of data and the cost-effective nature of data warehouses, implementing tools such as Looker enables folks from across the business to self-serve data as needed rather than having to go through analysts for reports and dashboards. Looker dashboards and reports can be leveraged internally or even embedded in external applications for external consumption. Whether you target this toward customers, partners, or internal teams, visualization and reporting tools are a great way to communicate with folks to help them understand the data that is available and extract value from it.

Now that we’ve followed along the data journey from ingestion through to visualization, let’s go back through the main components by way of a case study.